{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db5629a4-eb54-4145-884b-fb2c204573ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# Set style for visualizations\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d5dc16-a7d7-40e1-9fd5-18eb170a5ebe",
   "metadata": {},
   "source": [
    "<b> Product Matching System with validation (Testing, Metrics) and visualizations </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dc806ff-c0ae-46f2-848b-62198a777476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating test set\n",
    "matches_df = pd.read_csv('matching_results.csv')\n",
    "\n",
    "test_set = [\n",
    "    # Format: (Description_ID, Correct_SKU, Description, Notes)\n",
    "    ('DESC0001', 'SKU1000010', 'Style Pants for Fall 2025', 'Style brand, Parka (closest to pants)'),\n",
    "    ('DESC0009', 'SKU1000036', 'White Polo from Alpine, size M', 'Alpine, White, M'),\n",
    "    ('DESC0015', 'SKU1000023', 'Nordic Vest in Green size M', 'Exact match'),\n",
    "    ('DESC0017', 'SKU1000035', 'Comfort Coat in Beige size XXL', 'Beige Vest XXL (closest)'),\n",
    "    ('DESC0023', 'SKU1000004', 'Tan Parka from Premium, size XXL', 'Premium brand issue'),\n",
    "    ('DESC0031', 'SKU1000006', 'Alpine Parka Gray size L', 'Close match')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fd82340-fb35-437f-a0e4-754e7f568bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set created with 6 samples\n",
      "\n",
      "Sample test entries:\n",
      "  Description_ID Ground_Truth_SKU                       Description  \\\n",
      "0       DESC0001       SKU1000010         Style Pants for Fall 2025   \n",
      "1       DESC0009       SKU1000036    White Polo from Alpine, size M   \n",
      "2       DESC0015       SKU1000023       Nordic Vest in Green size M   \n",
      "3       DESC0017       SKU1000035    Comfort Coat in Beige size XXL   \n",
      "4       DESC0023       SKU1000004  Tan Parka from Premium, size XXL   \n",
      "\n",
      "                                   Notes  \n",
      "0  Style brand, Parka (closest to pants)  \n",
      "1                       Alpine, White, M  \n",
      "2                            Exact match  \n",
      "3               Beige Vest XXL (closest)  \n",
      "4                    Premium brand issue  \n"
     ]
    }
   ],
   "source": [
    "# Create test DataFrame\n",
    "test_df = pd.DataFrame(test_set, columns=['Description_ID', 'Ground_Truth_SKU', 'Description', 'Notes'])\n",
    "\n",
    "print(f\"Test set created with {len(test_df)} samples\")\n",
    "print(\"\\nSample test entries:\")\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e4c2cb9-2331-4b19-a947-276fe4faf35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set saved to 'test_set_ground_truth.csv'\n"
     ]
    }
   ],
   "source": [
    "# Save test set\n",
    "test_df.to_csv('test_set_ground_truth.csv', index=False)\n",
    "print(\"\\nTest set saved to 'test_set_ground_truth.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "409a178a-f3e7-4091-8421-ccc8c1f95780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate comprehensive performance metrics\n",
    "def calculate_metrics(test_df, matches_df):    \n",
    "    metrics = {\n",
    "        'top1_correct': 0,\n",
    "        'top3_correct': 0,\n",
    "        'total': len(test_df),\n",
    "        'confidence_scores': [],\n",
    "        'correct_confidences': [],\n",
    "        'incorrect_confidences': []\n",
    "    }    \n",
    "    detailed_results = []\n",
    "    \n",
    "    for idx, row in test_df.iterrows():\n",
    "        desc_id = row['Description_ID']\n",
    "        ground_truth = row['Ground_Truth_SKU']        \n",
    "        # Get top 3 matches for this description\n",
    "        desc_matches = matches_df[matches_df['Description_ID'] == desc_id].sort_values('Match_Rank')\n",
    "        if len(desc_matches) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Top-1 prediction\n",
    "        top1_sku = desc_matches.iloc[0]['SKU']\n",
    "        top1_conf = desc_matches.iloc[0]['Confidence']\n",
    "        top1_correct = (top1_sku == ground_truth)\n",
    "        \n",
    "        if top1_correct:\n",
    "            metrics['top1_correct'] += 1\n",
    "            metrics['correct_confidences'].append(top1_conf)\n",
    "        else:\n",
    "            metrics['incorrect_confidences'].append(top1_conf)\n",
    "        \n",
    "        # Top-3 prediction\n",
    "        top3_skus = desc_matches['SKU'].tolist()[:3]\n",
    "        top3_correct = ground_truth in top3_skus\n",
    "        \n",
    "        if top3_correct:\n",
    "            metrics['top3_correct'] += 1\n",
    "        \n",
    "        metrics['confidence_scores'].append(top1_conf)\n",
    "        \n",
    "        detailed_results.append({\n",
    "            'Description_ID': desc_id,\n",
    "            'Ground_Truth': ground_truth,\n",
    "            'Predicted_SKU': top1_sku,\n",
    "            'Top1_Correct': top1_correct,\n",
    "            'Top3_Correct': top3_correct,\n",
    "            'Confidence': top1_conf,\n",
    "            'Top3_SKUs': ', '.join(top3_skus)\n",
    "        })    \n",
    "    # Calculate percentages\n",
    "    metrics['top1_accuracy'] = (metrics['top1_correct'] / metrics['total']) * 100\n",
    "    metrics['top3_accuracy'] = (metrics['top3_correct'] / metrics['total']) * 100\n",
    "    metrics['avg_confidence'] = np.mean(metrics['confidence_scores'])\n",
    "    metrics['avg_correct_confidence'] = np.mean(metrics['correct_confidences']) if metrics['correct_confidences'] else 0\n",
    "    metrics['avg_incorrect_confidence'] = np.mean(metrics['incorrect_confidences']) if metrics['incorrect_confidences'] else 0\n",
    "    \n",
    "    return metrics, pd.DataFrame(detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddf78f80-aaec-4b9f-b1cd-8bbe7d95a843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERFORMANCE METRICS\n",
      "Total Test Cases: 6\n",
      "\n",
      "Top-1 Accuracy: 16.67%\n",
      "Top-3 Accuracy: 16.67%\n",
      "\n",
      "Average Confidence: 82.42%\n",
      "Avg Confidence (Correct): 89.37%\n",
      "Avg Confidence (Incorrect): 81.03%\n",
      "\n",
      "Detailed validation results saved to 'validation_results.csv'\n"
     ]
    }
   ],
   "source": [
    "# Calculate metrics\n",
    "performance_metrics, detailed_results = calculate_metrics(test_df, matches_df)\n",
    "\n",
    "print(\"PERFORMANCE METRICS\")\n",
    "print(f\"Total Test Cases: {performance_metrics['total']}\")\n",
    "print(f\"\\nTop-1 Accuracy: {performance_metrics['top1_accuracy']:.2f}%\")\n",
    "print(f\"Top-3 Accuracy: {performance_metrics['top3_accuracy']:.2f}%\")\n",
    "print(f\"\\nAverage Confidence: {performance_metrics['avg_confidence']:.2f}%\")\n",
    "print(f\"Avg Confidence (Correct): {performance_metrics['avg_correct_confidence']:.2f}%\")\n",
    "print(f\"Avg Confidence (Incorrect): {performance_metrics['avg_incorrect_confidence']:.2f}%\")\n",
    "\n",
    "# Save detailed results\n",
    "detailed_results.to_csv('validation_results.csv', index=False)\n",
    "print(\"\\nDetailed validation results saved to 'validation_results.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cd37cf-b5cd-47b8-af6f-a80b605595eb",
   "metadata": {},
   "source": [
    "<b> Let's investigate the error analysis </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce63a693-eb2d-400c-9d10-d6aca9bfdef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Errors: 5\n",
      "\n",
      "Error Cases:\n",
      "  Description_ID Ground_Truth Predicted_SKU  Confidence\n",
      "0       DESC0001   SKU1000010    SKU1000425       82.56\n",
      "1       DESC0009   SKU1000036    SKU1000182       73.21\n",
      "3       DESC0017   SKU1000035    SKU1000109       88.55\n",
      "4       DESC0023   SKU1000004    SKU1000142       71.05\n",
      "5       DESC0031   SKU1000006    SKU1000046       89.79\n"
     ]
    }
   ],
   "source": [
    "# Analyze errors\n",
    "errors_df = detailed_results[detailed_results['Top1_Correct'] == False]\n",
    "\n",
    "print(f\"\\nTotal Errors: {len(errors_df)}\")\n",
    "print(\"\\nError Cases:\")\n",
    "print(errors_df[['Description_ID', 'Ground_Truth', 'Predicted_SKU', 'Confidence']])\n",
    "\n",
    "# Error categorization\n",
    "error_analysis = {\n",
    "    'wrong_brand': 0,\n",
    "    'wrong_color': 0,\n",
    "    'wrong_size': 0,\n",
    "    'wrong_category': 0,\n",
    "    'ambiguous_description': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b1e28e8-9e07-41da-86ae-8063d1ec39c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load catalog for comparison\n",
    "catalog_df = pd.read_csv('b_product_catalog.csv')\n",
    "\n",
    "for idx, error in errors_df.iterrows():\n",
    "    desc_id = error['Description_ID']\n",
    "    # Analyze why it failed - you'd need to implement detailed logic here\n",
    "    # For now, we'll categorize based on confidence\n",
    "    if error['Confidence'] < 40:\n",
    "        error_analysis['ambiguous_description'] += 1\n",
    "    elif error['Confidence'] < 60:\n",
    "        error_analysis['wrong_category'] += 1\n",
    "    else:\n",
    "        error_analysis['wrong_brand'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89e47937-6b18-4e3c-b022-3a2e6f6c74cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error Breakdown:\n",
      "  wrong_brand: 5\n",
      "  wrong_color: 0\n",
      "  wrong_size: 0\n",
      "  wrong_category: 0\n",
      "  ambiguous_description: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nError Breakdown:\")\n",
    "for error_type, count in error_analysis.items():\n",
    "    print(f\"  {error_type}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a52add-bee1-4984-add1-3380dcd2af4d",
   "metadata": {},
   "source": [
    "<b> Let's analyze results by category </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37642c1d-3f29-4045-b5b3-440e8d23fc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with catalog to get categories\n",
    "results_with_cat = detailed_results.merge(\n",
    "    catalog_df[['SKU', 'Subcategory']], \n",
    "    left_on='Ground_Truth', \n",
    "    right_on='SKU',\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78eedccb-649e-4318-bdbb-c3f7d84b5554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performance by Subcategory:\n",
      "             Top1_Correct  Top3_Correct  Confidence\n",
      "Subcategory                                        \n",
      "Jacket                0.0           0.0      8150.0\n",
      "Parka                 0.0           0.0      7681.0\n",
      "Vest                 50.0          50.0      8896.0\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy by subcategory\n",
    "category_performance = results_with_cat.groupby('Subcategory').agg({\n",
    "    'Top1_Correct': 'mean',\n",
    "    'Top3_Correct': 'mean',\n",
    "    'Confidence': 'mean'\n",
    "}).round(2) * 100\n",
    "\n",
    "print(\"\\nPerformance by Subcategory:\")\n",
    "print(category_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f496204-30ed-4335-ba3a-1432119365d0",
   "metadata": {},
   "source": [
    "<b> cretating visualizations </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0570954-2589-4f8d-8534-d0995f94c3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Visualization 1 saved: viz1_confidence_distribution.png\n"
     ]
    }
   ],
   "source": [
    "# First visulization : Confidence Distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# All matches confidence distribution\n",
    "all_top1 = matches_df[matches_df['Match_Rank'] == 1]\n",
    "axes[0].hist(all_top1['Confidence'], bins=20, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(performance_metrics['avg_confidence'], color='red', linestyle='--', \n",
    "                label=f'Mean: {performance_metrics[\"avg_confidence\"]:.1f}%')\n",
    "axes[0].set_xlabel('Confidence Score (%)', fontsize=12)\n",
    "axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0].set_title('Distribution of Confidence Scores (All Matches)', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Correct vs Incorrect confidence\n",
    "axes[1].hist([performance_metrics['correct_confidences'], \n",
    "              performance_metrics['incorrect_confidences']], \n",
    "             bins=15, label=['Correct Matches', 'Incorrect Matches'],\n",
    "             color=['green', 'red'], alpha=0.6, edgecolor='black')\n",
    "axes[1].set_xlabel('Confidence Score (%)', fontsize=12)\n",
    "axes[1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1].set_title('Confidence: Correct vs Incorrect Matches', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('viz1_confidence_distribution.png', dpi=300, bbox_inches='tight')\n",
    "print(\"✓ Visualization 1 saved: viz1_confidence_distribution.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5255c19-f1cb-44db-ae7a-94d53b0dee37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Visualization 2 saved: viz2_accuracy_metrics.png\n"
     ]
    }
   ],
   "source": [
    "# Second visualization: Accuracy Metrics\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "metrics_data = {\n",
    "    'Top-1 Accuracy': performance_metrics['top1_accuracy'],\n",
    "    'Top-3 Accuracy': performance_metrics['top3_accuracy'],\n",
    "    'Avg Confidence': performance_metrics['avg_confidence']\n",
    "}\n",
    "\n",
    "bars = ax.bar(metrics_data.keys(), metrics_data.values(), \n",
    "              color=['#2ecc71', '#3498db', '#f39c12'], \n",
    "              edgecolor='black', linewidth=1.5)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height:.1f}%',\n",
    "            ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('Percentage (%)', fontsize=12)\n",
    "ax.set_title('Model Performance Metrics', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim(0, 100)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('viz2_accuracy_metrics.png', dpi=300, bbox_inches='tight')\n",
    "print(\"✓ Visualization 2 saved: viz2_accuracy_metrics.png\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5e4be07-4557-4ca0-b01e-1b6a4fd1472b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Visualization 3 saved: viz3_error_analysis.png\n"
     ]
    }
   ],
   "source": [
    "# Third visualization: Error Analysis\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "error_types = list(error_analysis.keys())\n",
    "error_counts = list(error_analysis.values())\n",
    "\n",
    "colors = ['#e74c3c', '#e67e22', '#f39c12', '#3498db', '#9b59b6']\n",
    "wedges, texts, autotexts = ax.pie(error_counts, labels=error_types, autopct='%1.1f%%',\n",
    "                                    colors=colors, startangle=90,\n",
    "                                    textprops={'fontsize': 11})\n",
    "\n",
    "# Make percentage text bold\n",
    "for autotext in autotexts:\n",
    "    autotext.set_color('white')\n",
    "    autotext.set_fontweight('bold')\n",
    "\n",
    "ax.set_title('Error Analysis by Type', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('viz3_error_analysis.png', dpi=300, bbox_inches='tight')\n",
    "print(\"✓ Visualization 3 saved: viz3_error_analysis.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6648879-3ed3-49f4-91da-cb02427359f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
