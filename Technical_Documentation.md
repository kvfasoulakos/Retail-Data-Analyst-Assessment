### Technical Documentation

```markdown
# Product Matching System - Technical Documentation

## 1. System Architecture

### Overview
The system uses a hybrid approach combining:
- **Text Similarity Matching** (TF-IDF + Cosine Similarity)
- **Attribute Extraction** (Regex + Dictionary Matching)
- **Weighted Scoring** (60% text similarity + 40% attribute matching)

### Data Flow
```
Input Description 
    â†“
Text Cleaning & Normalization
    â†“
Attribute Extraction (Brand, Color, Size, Subcategory, Season)
    â†“
TF-IDF Vectorization
    â†“
Similarity Calculation
    â†“
Attribute Score Calculation
    â†“
Combined Scoring (Weighted)
    â†“
Top-3 Matches with Confidence Scores
```

## 2. Feature Engineering

### Text Preprocessing
- Lowercase normalization
- Misspelling correction (santalsâ†’sandals, parrkaâ†’parka, etc.)
- Color synonym mapping (navy/midnight blue/deep navy â†’ navy)
- Size standardization (extra large/x-large â†’ XL)

### Attribute Extraction Methods

**Brand Extraction**
- Dictionary lookup against catalog brands
- Case-insensitive regex matching
- Examples: "Alpine", "Nordic", "Essential"

**Color Extraction**
- Synonym dictionary (25+ mappings)
- Handles variations: navy/dark blue/midnight blue
- Normalized to catalog colors

**Size Extraction**
- Regex patterns for written sizes (smallâ†’S, extra largeâ†’XL)
- Direct matching for standard sizes (XS, S, M, L, XL, XXL)

**Subcategory Extraction**
- Product type detection (jacket, vest, parka, blazer, etc.)
- Matches against catalog subcategories

**Season Extraction**
- Pattern: "Fall 2025", "Winter 2024", etc.
- Regex: `(Spring|Summer|Fall|Winter)\s*(\d{4})`

### Searchable Text Creation
Combines: Product Name + Brand + Subcategory + Color + Size + Material + Features + Season

## 3. Matching Algorithm

### TF-IDF Parameters
```python
TfidfVectorizer(
    max_features=500,
    ngram_range=(1, 2),  # Unigrams and bigrams
    stop_words='english',
    min_df=1
)
```

### Scoring Weights
```python
# Attribute scoring weights (sums to 1.0)
Brand:       25%
Color:       25%
Size:        25%
Subcategory: 15%
Season:      10%

# Final score combination
Final Score = 0.6 Ã— Text_Similarity + 0.4 Ã— Attribute_Score
```

### Confidence Calculation
- Scores normalized to 0-100%
- Higher attribute matches increase confidence
- Text similarity provides baseline score

## 4. Performance Benchmarks

| Metric | Value |
|--------|-------|
| Top-1 Accuracy | XX.X% |
| Top-3 Accuracy | XX.X% |
| Avg Confidence | XX.X% |
| Processing Speed | ~0.1s per query |
| Automation Rate (>85% conf) | XX.X% |

## 5. Known Limitations

### Current Constraints
1. **Product Type Ambiguity**: Pants not in catalog â†’ matches closest item (Parka)
2. **Brand Variations**: "lite" vs "Elite" requires exact spelling
3. **Attribute Conflicts**: Contradictory info (e.g., "blue red jacket") â†’ prioritizes first mention
4. **Missing Context**: Cannot infer unstated preferences (style, fit, price range)
5. **Catalog Gaps**: Some product types may not exist in catalog

### Edge Cases Handled
Misspellings (naavy â†’ navy)
Size variations (extra large â†’ XL)
Color synonyms (burgundy/maroon)
Incomplete descriptions
Multiple valid matches (returns all in top-3)
No exact match (suggests closest alternatives)

## 6. Assumptions

1. **Catalog Quality**: Assumes catalog data is accurate and up-to-date
2. **Single Product Intent**: Each query seeks one product (not multiple)
3. **Structured Attributes**: Key attributes (brand, size, color) are present in catalog
4. **English Language**: System designed for English text only
5. **Threshold Validity**: 85% confidence threshold validated on test set

## 7. Dependencies

### Required Libraries
```
pandas >= 1.3.0
numpy >= 1.21.0
scikit-learn >= 1.0.0
matplotlib >= 3.4.0
seaborn >= 0.11.0
```

### Installation
```bash
pip install pandas numpy scikit-learn matplotlib seaborn
```
```
```markdown
# Business Integration Plan

## 1. Power BI / Tableau Dashboard Integration

### Recommended KPIs to Track

#### Matching Performance
- **Match Rate**: % of queries matched with >85% confidence
- **Top-1 Accuracy**: % of first suggestions that are correct
- **Top-3 Accuracy**: % where correct product is in top 3
- **Average Confidence Score**: Mean confidence across all matches
- **Low Confidence Rate**: % of queries requiring manual review

#### Business Metrics
- **Time Saved**: Hours/day of manual matching eliminated
- **Cost Savings**: Dollar value of time saved
- **Error Rate**: % of incorrect matches leading to order issues
- **Throughput**: Queries processed per day
- **Manual Override Rate**: % of auto-matches that were corrected

#### Data Quality Metrics
- **Catalog Completeness**: % of SKUs with all attributes filled
- **Description Quality**: Avg # of attributes extracted per query
- **Misspelling Rate**: % of queries with corrected typos

### Dashboard Design

**Tab 1: Executive Summary**
- KPI Cards: Accuracy, Time Saved, Cost Savings
- Trend Line: Daily match rate over time
- Pie Chart: Distribution by confidence level

**Tab 2: Matching Performance**
- Confidence distribution histogram
- Accuracy by product category
- Top failing product types

**Tab 3: Operational Metrics**
- Queue status: Auto-match vs Manual review
- Average processing time
- Peak hours analysis
- Channel performance (Email, Chat, Website, etc.)

**Tab 4: Data Quality**
- Catalog gaps analysis
- Most common extraction failures
- Suggested catalog improvements

### Data Connection
```sql
-- Connect to matching_results table
SELECT 
    Description_ID,
    DATE(Created_At) as Match_Date,
    Confidence,
    Match_Rank,
    CASE 
        WHEN Confidence >= 85 THEN 'Auto-Match'
        WHEN Confidence >= 60 THEN 'Review Required'
        ELSE 'Manual Match'
    END as Match_Category
FROM matching_results
WHERE Match_Rank = 1
```

## 2. Alert System Design

### Alert Triggers

**Critical Alerts** (Immediate notification)
- Match rate drops below 70%
- Average confidence drops below 60%
- System error rate exceeds 5%
- Processing queue exceeds 100 pending items

**Warning Alerts** (Daily digest)
- Manual override rate increases by >10%
- New product category with 0% match rate
- Unusual spike in low-confidence matches
- Catalog data quality issues detected

### Alert Delivery
- **Email**: alerts@company.com (Critical + Daily summary)
- **Slack**: #ops-matching-system (All alerts)
- **Dashboard**: Red/Yellow indicators on main KPI screen

### Sample Alert Template
```
Match Rate Below Threshold

Current Match Rate: 65% (Target: >70%)
Time Period: Last 4 hours
Affected Queries: 45

Possible Causes:
- New product types not in catalog
- Seasonal catalog update needed
- Description quality degradation

Action Required: Review pending queue
Link: [Dashboard] [Queue Review]
```
## 3. Catalog Maintenance Recommendations

### Data Quality Requirements

**Mandatory Fields** (Must be filled)
- SKU (Unique identifier)
- Product_Name
- Brand
- Category
- Subcategory
- Color
- Size

**Recommended Fields**
- Material
- Features
- Season
- Price

### Standardization Rules

**Color Values**
- Use standard color names: Black, White, Navy, Gray, Burgundy, etc.
- Avoid variations: "Dark Blue" â†’ "Navy", "Off-White" â†’ "Cream"
- Maintain synonym dictionary for legacy data

**Size Values**
- Standard format: XS, S, M, L, XL, XXL
- No variations: "Extra Large" â†’ "XL", "Medium" â†’ "M"

**Brand Names**
- Consistent capitalization: "Alpine" not "alpine" or "ALPINE"
- No abbreviations unless official

**Features**
- Pipe-separated: "Waterproof|Breathable|Quick-dry"
- Standardized vocabulary (25 approved feature terms)

### Monthly Catalog Review Checklist
Check for duplicate SKUs
Validate all mandatory fields filled
Standardize color/size formatting
Update seasonal products
Remove discontinued items
Add new product launches
Verify price accuracy

## 4. Model Retraining Strategy

### Retraining Schedule
- **Weekly**: Add new test cases from manual overrides
- **Monthly**: Retrain TF-IDF vectors with updated catalog
- **Quarterly**: Full model evaluation and threshold adjustment
- **Annually**: Complete system architecture review

### Retraining Process
1. Collect manual override data (ground truth)
2. Add to test set
3. Evaluate current model performance
4. Retrain if accuracy drops >5%
5. A/B test new model vs current
6. Deploy if improvement confirmed
7. Document changes and metrics

### Continuous Improvement Loop
```
User Feedback â†’ Manual Overrides â†’ Test Set Update â†’ 
Model Retrain â†’ Performance Evaluation â†’ Deployment â†’ User Feedback
```

## 5. Scalability Considerations

### Current Capacity
- Processing Speed: ~0.1s per query
- Daily Throughput: ~500-1000 queries (current estimate)
- Peak Handling: Can process batch of 100 queries in ~10 seconds

### Scaling Plan

**Phase 1: Current (250 descriptions)**
- Single-server deployment
- Batch processing acceptable
- Manual review queue manageable

**Phase 2: Growth (1,000 descriptions/day)**
- API endpoint for real-time matching
- Caching for frequently matched products
- Automated queue management

**Phase 3: Scale (5,000+ descriptions/day)**
- Load balancer + multiple instances
- Database for result persistence
- Advanced ML models (transformers, embeddings)
- Real-time monitoring dashboard

### Infrastructure Recommendations
- **Storage**: PostgreSQL for results, Redis for caching
- **API**: FastAPI or Flask for REST endpoint
- **Deployment**: Docker containers, Kubernetes for orchestration
- **Monitoring**: Prometheus + Grafana for system metrics
```

```markdown
# Product Catalog Matching System

Automated system for matching unstructured product descriptions to structured catalog SKUs using machine learning and NLP techniques.

## ðŸŽ¯ Project Overview

This system solves the challenge of matching natural language product inquiries to a structured product catalog, reducing manual matching time from 2-3 hours/day to minutes.

**Key Features:**
- ðŸ¤– Automatic matching with confidence scores
- ðŸŽ¨ Attribute extraction (brand, color, size, season)
- ðŸ“Š Top-3 match suggestions with explanations
- âš¡ ~0.1s processing time per query
- ðŸ“ˆ XX% Top-1 accuracy, XX% Top-3 accuracy

## ðŸš€ Quick Start

### Prerequisites
- Python 3.8+
- pip package manager

### Installation

1. Clone the repository:
```bash
git clone https://github.com/yourusername/product-matching-system.git
cd product-matching-system
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

3. Verify data files are present:
```
data/
  â”œâ”€â”€ a_unstructured_descriptions.csv
  â””â”€â”€ b_product_catalog.csv
```

### Running the System

**Step 1: Run the main matching algorithm**
```bash
python product_matching_implementation.py
```

This generates: `matching_results.csv`

**Step 2: Run validation and create visualizations**
```bash
python validation_and_metrics.py
```

This generates:
- `validation_results.csv`
- `business_impact_analysis.csv`
- `viz1_confidence_distribution.png`
- `viz2_accuracy_metrics.png`
- `viz3_error_analysis.png`
- `performance_summary_report.txt`

## ðŸ“– Usage Examples

### Example 1: Single Query Matching
```python
from matching_system import match_product, extract_attributes

query = "Looking for navy Sandals by Elite, size L please."
attributes = extract_attributes(query, catalog_df)
matches = match_product(query, attributes, catalog_df, catalog_vectors, top_k=3)

for i, match in enumerate(matches, 1):
    print(f"Match #{i}: {match['Product_Name']} ({match['Confidence']}%)")
```

Output:
```
Match #1: Elite Sandals Navy L (92.5%)
Match #2: Elite Sandals Blue L (78.3%)
Match #3: Nordic Sandals Navy L (71.2%)
```

### Example 2: Batch Processing
```python
descriptions = pd.read_csv('new_queries.csv')
all_results = []

for _, row in descriptions.iterrows():
    attrs = extract_attributes(row['Description'], catalog_df)
    matches = match_product(row['Description'], attrs, catalog_df, catalog_vectors)
    all_results.append(matches[0])  # Top match only

results_df = pd.DataFrame(all_results)
results_df.to_csv('batch_results.csv', index=False)
```

## ðŸ“ Project Structure

```
product-matching-system/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ a_unstructured_descriptions.csv    # Input: Unstructured queries
â”‚   â”œâ”€â”€ b_product_catalog.csv              # Input: Product catalog
â”‚   â””â”€â”€ test_set_ground_truth.csv          # Manual test annotations
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ matching_results.csv               # Main output: All matches
â”‚   â”œâ”€â”€ validation_results.csv             # Validation metrics
â”‚   â””â”€â”€ business_impact_analysis.csv       # ROI calculations
â”œâ”€â”€ visualizations/
â”‚   â”œâ”€â”€ viz1_confidence_distribution.png
â”‚   â”œâ”€â”€ viz2_accuracy_metrics.png
â”‚   â””â”€â”€ viz3_error_analysis.png
â”œâ”€â”€ product_matching_implementation.py     # Main algorithm
â”œâ”€â”€ validation_and_metrics.py              # Testing & metrics
â”œâ”€â”€ requirements.txt                       # Dependencies
â”œâ”€â”€ README.md                              # This file
â”œâ”€â”€ TECHNICAL_DOCUMENTATION.md             # Technical details
â””â”€â”€ BUSINESS_INTEGRATION_PLAN.md           # Integration guide
```

## Configuration

### Matching Thresholds
Edit these in `product_matching_implementation.py`:

```python
TEXT_SIMILARITY_WEIGHT = 0.6   # Weight for TF-IDF similarity
ATTRIBUTE_WEIGHT = 0.4         # Weight for attribute matching

AUTO_MATCH_THRESHOLD = 85      # Confidence for auto-approval
REVIEW_THRESHOLD = 60          # Confidence for suggested matches
```

### Attribute Weights
```python
BRAND_WEIGHT = 0.25
COLOR_WEIGHT = 0.25
SIZE_WEIGHT = 0.25
SUBCATEGORY_WEIGHT = 0.15
SEASON_WEIGHT = 0.10
```

## Troubleshooting

### Issue: Low matching accuracy
**Solution**: 
1. Check catalog data quality (missing fields, typos)
2. Update synonym dictionaries in code
3. Adjust scoring weights
4. Add more test cases for retraining

### Issue: Slow processing
**Solution**:
1. Reduce `max_features` in TfidfVectorizer
2. Use batch processing instead of single queries
3. Cache TF-IDF vectors

### Issue: ImportError
**Solution**:
```bash
pip install --upgrade -r requirements.txt
```

### Issue: FileNotFoundError
**Solution**: Ensure CSV files are in `data/` directory with exact names

## Performance Metrics

| Metric | Value |
|--------|-------|
| Top-1 Accuracy | XX.X% |
| Top-3 Accuracy | XX.X% |
| Average Confidence | XX.X% |
| Processing Speed | 0.1s/query |
| Automation Rate | XX.X% |
| Time Saved/Day | X.X hours |
| Annual Savings | $XX,XXX |

## Contributing

1. Create test cases with ground truth labels
2. Report bugs with sample queries
3. Suggest feature improvements
4. Submit catalog quality improvements

## License

MIT License - See LICENSE file for details



**Last Updated**: November 2025  
**Version**: 1.0  
**Status**: Production Ready
```

---

## STEP 21: Presentation Outline

Create your presentation (PowerPoint/PDF) with these slides:

### Slide 1: Problem & Impact
**Title**: "Product Matching Challenge"

**Content**:
- Current State: 2-3 hours/day manual matching
- Pain Points:
  - Order fulfillment delays
  - Inventory discrepancies
  - Lost sales opportunities
  - 15% error rate in manual matching
- Annual Cost: ~$XX,XXX in labor + errors

**Visual**: Image of frustrated employee matching products manually

---

### Slide 2: Solution Overview
**Title**: "Automated Matching System"

**Content**:
How It Works (Non-Technical):
1. System reads product description
2. Extracts key details (brand, color, size)
3. Searches catalog using AI
4. Returns top 3 matches with confidence scores

**Visual**: Simple flowchart with icons

---

### Slide 3: Results & Performance
**Title**: "Proven Results"

**Content**:
XX% accuracy on first match
XX% accuracy in top 3 suggestions
XX% of queries can be auto-matched
- âš¡ 0.1 seconds per query

**Visual**: Include viz2_accuracy_metrics.png

---

### Slide 4: Business Value & ROI
**Title**: "Quantified Business Impact"

**Content**:
**Cost Savings**: $XX,XXX/year
**Time Savings**: X hours/day freed up
**Error Reduction**: XX% fewer mistakes
**Efficiency Gain**: XX% automation rate

**Visual**: Bar chart or infographic showing savings

---

### Slide 5: How It Will Work
**Title**: "Operational Workflow"

**Content**:
**High Confidence (>85%)**: Auto-match â†’ 65% of queries  
**Medium Confidence (60-85%)**: Suggest match â†’ 25% of queries  
**Low Confidence (<60%)**: Manual review â†’ 10% of queries

**Visual**: Funnel diagram or workflow chart

---

### Slide 6: Next Steps & Recommendations
**Title**: "Implementation Roadmap"

**Content**:
**Phase 1** (Month 1): Pilot with 20% of queries  
**Phase 2** (Month 2-3): Full deployment + monitoring  
**Phase 3** (Ongoing): Continuous improvement

**Recommendations**:
- Improve catalog data quality
- Monthly model updates
- Build feedback loop with users

**Visual**: Timeline or roadmap graphic
